# -*- coding: utf-8 -*-
"""Group19_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pqERqvpCPGokxFsACjjFLJI-PbNp0bdZ

**Importing neccessary features that would be needed during the execution of the program**
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from google.colab import drive
drive.mount('/content/drive')

"""**Loading the data sets that will be used in training and testing i.e player_21 and player_22**"""

df_train = pd.read_csv('/content/drive/My Drive/AI Project/players_21.csv')
df_test = pd.read_csv('/content/drive/My Drive/AI Project/players_22.csv')

"""**Dropping uneccessary columns in the dataframes**"""

columns_to_drop = ['player_url', 'player_positions', 'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url']
df_train = df_train.drop(columns=columns_to_drop)
df_test = df_test.drop(columns=columns_to_drop)

"""Inspecting the initial rows of our DataFrame. This allows us to see the first few rows of our dataset, helping us understand the structure of the data and the types of values present in each column. It gives us a sense of what the data looks like and will help us decide on further analysis and preprocessing steps."""

df_train

df_test

"""Dropping columns in our dataset that have more than 30% missing values"""

# Drop columns with more than 30% missing values
missing_percentage_train = df_train.isnull().sum() / len(df_train) * 100
columns_to_drop_train = missing_percentage_train[missing_percentage_train > 30].index
df_train.drop(columns_to_drop_train, axis=1, inplace=True)

"""These are the columns that have been dropped in the datasets"""

columns_to_drop_train

df_train.info()

"""Dropping columns with more than 30% missing values in player_22

---


"""

missing_percentage_test = df_test.isnull().sum() / len(df_test) * 100
columns_to_drop_test = missing_percentage_test[missing_percentage_test > 30].index
df_test.drop(columns_to_drop_test, axis=1, inplace=True)

columns_to_drop_test

df_test.info()

"""Separating data into numeric and non_numeric features in both datasets"""

numeric_features_train = df_train.select_dtypes(include=['number'])
non_numeric_features_train = df_train.select_dtypes(exclude=['number'])

numeric_features_test = df_test.select_dtypes(include=['number'])
non_numeric_features_test = df_test.select_dtypes(exclude=['number'])

numeric_features_train.info() , non_numeric_features_train.info()

numeric_features_test.info() , non_numeric_features_test.info()

"""Imputing missing values in the numeric and non numeric columns for both data sets"""

numeric_imputer = SimpleImputer(strategy='mean')
df_train[numeric_features_train.columns] = numeric_imputer.fit_transform(df_train[numeric_features_train.columns])
df_test[numeric_features_test.columns] = numeric_imputer.transform(df_test[numeric_features_test.columns])

non_numeric_imputer = SimpleImputer(strategy='most_frequent')
df_train[non_numeric_features_train.columns] = non_numeric_imputer.fit_transform(df_train[non_numeric_features_train.columns])
df_test[non_numeric_features_test.columns] = non_numeric_imputer.transform(df_test[non_numeric_features_test.columns])

df_train.info()

df_test.info()

"""Encoding the categorical values in the non numeric columns using label encoder


"""

label_encoder = LabelEncoder()
for column in non_numeric_features_train.columns:
    df_train[column] = label_encoder.fit_transform(df_train[column])
for column in non_numeric_features_test.columns:
    df_test[column] = label_encoder.fit_transform(df_test[column])

"""Displaying first 5 rows of the encoded data"""

df_train.head()

df_test.head()

"""**PLAYER 21 FEATURE SELECTION AND SCALING**

Separating features and target variable for training the data from the player 21 data set.
"""

X_train = df_train.drop('overall', axis=1)
y_train = df_train['overall']

"""training a random forest regression model. It creates an instance of the RandomForestRegressor class and then fits the model to the training data (X_train and y_train)."""

rf_model_train = RandomForestRegressor()
rf_model_train.fit(X_train, y_train)

"""The above code is calculating the feature importances of a random forest model (`rf_model_train`) trained on a training dataset (`X_train`). It then creates a DataFrame (`feature_importance_df_train`) to store the feature importances, with columns for the feature names and their corresponding importances. Finally, it sorts the features in descending order based on their importance values."""

feature_importances_train = rf_model_train.feature_importances_
feature_importance_df_train = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances_train})
feature_importance_df_train = feature_importance_df_train.sort_values(by='Importance', ascending=False)

"""selecting the top 7 features based on their importance scores and then creating a new dataframe called `top_features_train_importances` which contains the top 7 features along with their importance scores."""

top_features_train = feature_importance_df_train.head(7)['Feature'].tolist()
top_features_train_importances = feature_importance_df_train.head(7)

"""Displaying the top 7 features"""

top_features_train

"""Displaying top 7 features with their importance score"""

top_features_train_importances

"""Importing the StandardScaler class from the sklearn.preprocessing module.Then separating the selected features and the target variable from the training data. The selected features are stored in the variable X_train, and the target variable is stored in the variable y_train."""

from sklearn.preprocessing import StandardScaler

X_train = df_train[top_features_train]
y_train = df_train['overall']



"""performing feature scaling on the training data using the StandardScaler() function from the scikit-learn library
Then displaying the datafram with the scaled data(features)

"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

pd.DataFrame(X_train, columns=top_features_train)

"""**PLAYER 22 FEATURE SELECTION AND SCALING**

Separating features and target variable for training data (player_21)
"""

X_test_player22 = df_test.drop('overall', axis=1)
y_test_player22 = df_test['overall']

"""creating a new variable called "X_test_player22" and assigning it the value of a
subset of the "df_test" dataframe. The subset is created by selecting only the columns specified in the "top_features_train" variable.
"""

X_test_player22 = df_test[top_features_train]

X_test_player22

"""Here we are scaling the features of Player 22's test data using a scaler object called "scaler". It then displays the scaled features in a pandas DataFrame with column names specified by the "top_features_train" variable."""

X_test_player22_scaled = scaler.transform(X_test_player22)
pd.DataFrame(X_test_player22_scaled, columns=top_features_train)

""" Here we are splitting the dataset into training and testing sets. It uses the `train_test_split` function from the `sklearn.model_selection` module to randomly split the `X_train` and `y_train` datasets into `Xtrain`, `Xtest`, `ytrain`, and `ytest` sets. The `test_size` parameter is set to 0.2, which means that 20% of the data will be used for testing and 80% will be used for training."""

from sklearn.model_selection import train_test_split

Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

"""The function evaluates a machine learning model by calculating the mean absolute error (MAE) and R-squared (R2) score between the predicted and true values.[param model: The model is the machine learning model that has been trained and is being evaluated, param X: The input features or independent variables used to make predictions , param y_true: The true values of the target variable (the actual values you are trying to predict),
 return: the mean absolute error (MAE) and the R-squared (R2) score.]
"""

def evaluate_model(model, X, y_true):
    y_pred = model.predict(X)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return  mae, r2

"""performing hyperparameter tuning and evaluation of a RandomForestRegressor modele using cross-validation."""

rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)
rf_params = {'n_estimators': [100, 200,400], 'max_depth': [10,30,40]}
rf_grid = GridSearchCV(rf_model, rf_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
rf_grid.fit(Xtrain, ytrain)
rf_model = rf_grid.best_estimator_
cross_val_scores = cross_val_score(rf_model, Xtrain, ytrain, cv=5, scoring='neg_mean_absolute_error')
print("Cross-Validation MAE Scores:", -cross_val_scores)
rf_metrics = evaluate_model(rf_model, Xtest, ytest)
print('\nBest RandomForestRegressor Metrics (on Test Set):', rf_metrics)
print('\n')

"""performing hyperparameter tuning and evaluation of an XGBoost regression model"""

xgb_model = XGBRegressor(random_state=42, n_jobs=-1)
xgb_params = {'n_estimators': [50, 100, 200], 'max_depth': [2, 10, 20], 'learning_rate': [0.1, 0.001,0.01]}
xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
xgb_grid.fit(Xtrain, ytrain)
xgb_model = xgb_grid.best_estimator_
cross_val_scores_xgb = cross_val_score(xgb_model, Xtrain, ytrain, cv=5, scoring='neg_mean_absolute_error')
print("Cross-Validation MAE Scores for XGBoost:", -cross_val_scores_xgb)
xgb_metrics = evaluate_model(xgb_model, Xtest, ytest)
print('\nBest XGBoost Metrics (on Test Set):', xgb_metrics)
print('\n')

"""performing hyperparameter tuning for a Gradient Boosting Regressor model using GridSearchCV. It creates a GradientBoostingRegressor model with a random state of 42. It then defines a dictionary of hyperparameters to search over, including the number of estimators, maximum depth, and learning rate."""

gb_model = GradientBoostingRegressor(random_state=42)
gb_params = {'n_estimators': [50, 100, 200], 'max_depth': [2, 10, 20],'learning_rate': [0.1, 0.01,0.01]}
gb_grid = GridSearchCV(gb_model, gb_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
gb_grid.fit(Xtrain, ytrain)
gb_model = gb_grid.best_estimator_
cross_val_scores_gb = cross_val_score(gb_model, Xtrain, ytrain, cv=5, scoring='neg_mean_absolute_error')
print("Cross-Validation MAE Scores for Gradient Boosting:", -cross_val_scores_gb)
gb_metrics = evaluate_model(gb_model, Xtest, ytest)
print('\nBest Gradient Boosting Metrics (on Test Set):', gb_metrics)
print('\n')

"""The ensemble model combines the predictions of three individual models: rf_model, xgb_model, and gb_model. The ensemble model is trained on the Xtrain and ytrain data. After training, the model is evaluated using the evaluate_model function on the Xtest and ytest data. The evaluation metrics are then printed."""

from sklearn.model_selection import GridSearchCV

ensemble_model = VotingRegressor(estimators=[('rf', rf_model), ('xgb', xgb_model), ('gb', gb_model)], n_jobs=-1)
ensemble_params = {
    'weights': [[1, 1, 1], [2, 1, 1], [1, 2, 1], [1, 1, 2], [2, 2, 1], [2, 1, 2], [1, 2, 2]]
    }
grid_search_ensemble = GridSearchCV(ensemble_model, ensemble_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
grid_search_ensemble.fit(Xtrain, ytrain)
print("Best Parameters for VotingRegressor:", grid_search_ensemble.best_params_)
print("Best Cross-Validation MAE Score for VotingRegressor:", -grid_search_ensemble.best_score_)
best_ensemble_model = grid_search_ensemble.best_estimator_
ensemble_metrics_test = evaluate_model(best_ensemble_model, Xtest, ytest)
print('VotingRegressor Test Metrics:', ensemble_metrics_test)

"""calls the `evaluate_model` function, passing in the `rf_model` (RandomForestRegressor model), `X_test_player22_scaled` (scaled test dataset for Player 22), and `y_test_player22` (target variable for Player 22). The function returns the metrics for the model's performance, which are then printed out."""

rf_player22_metrics = evaluate_model(rf_model, X_test_player22_scaled, y_test_player22)
print('RandomForestRegressor Metrics on Player 22:', rf_player22_metrics)
print('\n')

"""Calling the function `evaluate_model` with the XGBoost model, the scaled test dataset (`X_test_player22_scaled`), and the corresponding labels(`y_test_player22`)."""

xgb_player22_metrics = evaluate_model(xgb_model, X_test_player22_scaled, y_test_player22)
print('XGBoost Metrics on Player 22:', xgb_player22_metrics)
print('\n')

"""The code is evaluating the performance of the GradientBoostingRegressor model on the Player 22 data. It uses the `evaluate_model` function to calculate the mean absolute error (MAE) and R-squared (R2) scores between the predicted values (`gb_model.predict(X_test_player22_scaled)`) and the actual values (`y_test_player22`). The calculated metrics are then stored in the `gb_player22_metrics` variable and printed out."""

gb_player22_metrics = evaluate_model(gb_model, X_test_player22_scaled, y_test_player22)
print('GradientBoostingRegressor Metrics on Player 22:', gb_player22_metrics)
print('\n')

"""calling the function `evaluate_model` with the ensemble model, the scaled test data (`X_test_player22_scaled`), and the corresponding target variable (`y_test_player22`). The function returns the metrics of the model's performance, which are then printed out.

"""

ensemble_player22_metrics = evaluate_model(best_ensemble_model, X_test_player22_scaled, y_test_player22)
print('VotingRegressor Metrics on Player 22:', ensemble_player22_metrics)
print('\n')

"""The code is using the trained models (RandomForestRegressor, XGBoost, GradientBoostingRegressor, and VotingRegressor) to make predictions on the Player 22 data."""

rf_predictions = rf_model.predict(X_test_player22_scaled)
xgb_predictions = xgb_model.predict(X_test_player22_scaled)
gb_predictions = gb_model.predict(X_test_player22_scaled)
ensemble_predictions = best_ensemble_model.predict(X_test_player22_scaled)

"""Calling the `evaluate_model` function for each model, passing in the respective model, the scaled features (`X_test_player22_scaled`), and the target variable (`y_test_player22`). The function returns the evaluation metrics for each model,
which are then printed out.
"""

rf_metrics_test = evaluate_model(rf_model, X_test_player22_scaled, y_test_player22)
xgb_metrics_test = evaluate_model(xgb_model, X_test_player22_scaled, y_test_player22)
gb_metrics_test = evaluate_model(gb_model, X_test_player22_scaled, y_test_player22)
ensemble_metrics_test = evaluate_model(best_ensemble_model, X_test_player22_scaled, y_test_player22)

print('RandomForestRegressor Test Metrics (player 22):', rf_metrics_test)
print ("  ")
print('XGBoost Test Metrics(player 22):', xgb_metrics_test)
print ("  ")
print('GradientBoostingRegressor Test Metrics(player 22):', gb_metrics_test)
print ("  ")
print('VotingRegressor Test Metrics:(player 22)', ensemble_metrics_test)

"""We ended up going for the random forest regressor model with grid search because it has a lower mean absolute score as compared to all the other models."""

import joblib

joblib.dump(rf_model, 'FifaPredictionmodel.pkl')

joblib.dump(label_encoder, 'label_encoder.pkl')

joblib.dump(scaler, 'Scaler.pkl')

import sklearn
print(sklearn.__version__)

